{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bbfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "\n",
    "import util.birdnet_util.audio as audio\n",
    "from util.birdnet_util.audio0 import spectrogram  # Spectrogram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9a1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LOAD TRAINED MODEL ---------------------- #\n",
    "def model_loading(MODEL_PATH, TFLITE_THREADS=1):\n",
    "    interpreter = tf.lite.Interpreter(model_path=MODEL_PATH, num_threads=TFLITE_THREADS)\n",
    "    interpreter.allocate_tensors()\n",
    "    print(\"[INFO] Model loaded successfully.\")\n",
    "    return interpreter\n",
    "\n",
    "def load_labels(LABEL_FILE):\n",
    "    with open(LABEL_FILE, \"r\") as f:\n",
    "       LABELS = [line.strip() for line in f]\n",
    "\n",
    "    print(f\"# Target categories: {len(LABELS)}\")\n",
    "    NUM_CLASSES = len(LABELS)\n",
    "    return LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e739ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded successfully.\n",
      "# Target categories: 337\n",
      "First labels:\n",
      "  Accipiter gentilis\n",
      "  Accipiter nisus\n",
      "  Acrocephalus agricola\n",
      "  Acrocephalus arundinaceus\n",
      "  Acrocephalus dumetorum\n",
      "  Acrocephalus paludicola\n",
      "  Acrocephalus palustris\n",
      "  Acrocephalus schoenobaenus\n",
      "  Acrocephalus scirpaceus\n",
      "  Actitis hypoleucos\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"mobilenet-224-337wi-ft.tflite\" \n",
    "LABEL_FILE = \"species-list-337.txt\"\n",
    "TFLITE_THREADS = max(1, multiprocessing.cpu_count() // 2)\n",
    "\n",
    "interpreter = model_loading(MODEL_PATH, TFLITE_THREADS)\n",
    "LABELS = load_labels(LABEL_FILE)\n",
    "\n",
    "print(\"First labels:\")\n",
    "for k in LABELS[:10]:\n",
    "    print(f\"  {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2b9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT FILE\n",
    "INPUT_PATH = \"./\"\n",
    "AUDIO_FILE = \"XC793531.MP3\"\n",
    "\n",
    "# PARAMETERS\n",
    "SAMPLE_RATE = 48000\n",
    "FILE_SPLITTING_DURATION = 600\n",
    "BANDPASS_FMIN = 0\n",
    "BANDPASS_FMAX = 15000\n",
    "SIG_LENGTH = 3.0\n",
    "SIG_OVERLAP = 0\n",
    "SIG_MINLEN = SIG_LENGTH\n",
    "MAX_LIMIT = 1000\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "rescaling = 1.0 / 255.0\n",
    "MAX_SEGMENTS = 1000\n",
    "MIN_CONF = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "958625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUXILIARY FUNCTIONS\n",
    "def apply_confidence_threshold(predictions, threshold=0.5, top_only=False):\n",
    "    filtered_predictions = {}\n",
    "\n",
    "    for k, pred in predictions.items():\n",
    "        filtered_predictions[k] = []\n",
    "\n",
    "        if threshold == -1:\n",
    "            max_index = np.argmax(pred)\n",
    "            max_conf = pred[max_index]\n",
    "            filtered_predictions[k].append((max_index, max_conf))\n",
    "        elif top_only:\n",
    "            max_index = np.argmax(pred)\n",
    "            max_conf = pred[max_index]\n",
    "            if max_conf >= threshold:\n",
    "                filtered_predictions[k].append((max_index, max_conf))\n",
    "        else:\n",
    "            for i, c in enumerate(pred):\n",
    "                if c > threshold:\n",
    "                    filtered_predictions[k].append((i, c))\n",
    "\n",
    "    return filtered_predictions\n",
    "\n",
    "def analyze_file(f):\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "\n",
    "    print(f\"Analyzing {f}\", flush=True)\n",
    "    start_time = datetime.datetime.now()\n",
    "    full_path = os.path.join(INPUT_PATH, f)\n",
    "    chunk_preds = []\n",
    "    print_preds = {}\n",
    "\n",
    "    sig, rate = audio.openAudioFile(full_path, SAMPLE_RATE, offset=0, duration=FILE_SPLITTING_DURATION, fmin=BANDPASS_FMIN, fmax=BANDPASS_FMAX)\n",
    "    chunks = audio.splitSignal(sig, rate, SIG_LENGTH, SIG_OVERLAP, SIG_MINLEN)\n",
    "\n",
    "    for interval, y in enumerate(chunks[:MAX_SEGMENTS]):\n",
    "                    spec, _ = spectrogram(y, rate, shape=(128, 224))\n",
    "                    try:\n",
    "                        standardized_spec = (spec - np.min(spec)) / (np.max(spec) - np.min(spec)) \n",
    "                    except RuntimeWarning:\n",
    "                        continue\n",
    "\n",
    "                    spec_array = (np.asarray(standardized_spec.T) * 255)\n",
    "                    img = Image.fromarray(spec_array.T)\n",
    "\n",
    "                    # Preprocessing\n",
    "                    img = img.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "                    img = np.expand_dims(img, axis=-1)  # channel dimension (1)\n",
    "                    img = np.repeat(img, 3, axis=-1)  # to 3-channel\n",
    "                    img = np.expand_dims(img, axis=0)  # add batch dimension\n",
    "                    img = img.astype(np.float32) * rescaling\n",
    "\n",
    "                    # Model inference\n",
    "                    interpreter.set_tensor(input_details[0]['index'], img.astype(input_details[0]['dtype']))\n",
    "                    interpreter.invoke()\n",
    "\n",
    "                    # Results\n",
    "                    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                    predictions = np.squeeze(output_data)\n",
    "                    predicted_class = np.argmax(predictions)\n",
    "\n",
    "                    print_preds[f\"{interval*SIG_LENGTH}-{(interval+1)*SIG_LENGTH}\"] = predictions\n",
    "\n",
    "    # Show predictions\n",
    "    filtered_predictions = apply_confidence_threshold(print_preds, MIN_CONF, top_only=False)\n",
    "    for k, p_c in filtered_predictions.items():\n",
    "                        predicted_class, confidence = p_c\n",
    "                        label = LABELS[predicted_class] if predicted_class < len(LABELS) else f\"class_{pred_class}\"\n",
    "                        print(f\"{k.split('-')[0]}\\t{k.split('-')[1]}\\t{LABELS[predicted_class]}\\t{confidence:.2f}\\t{f}\")\n",
    "\n",
    "\n",
    "    delta_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    print(f\"Finished {f} in {delta_time:.2f} seconds\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f89ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing XC793531.MP3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17948\\1021421735.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ---------------------- ANALYZE FILE ---------------------- #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manalyze_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAUDIO_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17948\\2712302345.py\u001b[0m in \u001b[0;36manalyze_file\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mfiltered_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_confidence_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprint_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMIN_CONF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_c\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                         \u001b[0mpredicted_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_c\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_class\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpredicted_class\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mf\"class_{pred_class}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{k.split('-')[0]}\\t{k.split('-')[1]}\\t{LABELS[predicted_class]}\\t{confidence:.2f}\\t{f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# ---------------------- ANALYZE FILE ---------------------- #\n",
    "analyze_file(AUDIO_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ad79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
