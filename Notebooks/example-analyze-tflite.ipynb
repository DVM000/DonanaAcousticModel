{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41bbfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "\n",
    "import birdnet_util.audio as audio\n",
    "from birdnet_util.audio0 import spectrogram  # Spectrogram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed9a1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LOAD TRAINED MODEL ---------------------- #\n",
    "def model_loading(MODEL_PATH, TFLITE_THREADS=1):\n",
    "    interpreter = tf.lite.Interpreter(model_path=MODEL_PATH, num_threads=TFLITE_THREADS)\n",
    "    interpreter.allocate_tensors()\n",
    "    print(\"[INFO] Model loaded successfully.\")\n",
    "    return interpreter\n",
    "\n",
    "def load_labels(LABEL_FILE):\n",
    "    with open(LABEL_FILE, \"r\") as f:\n",
    "       LABELS = [line.strip() for line in f]\n",
    "\n",
    "    print(f\"# Target categories: {len(LABELS)}\")\n",
    "    NUM_CLASSES = len(LABELS)\n",
    "    return LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68e739ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded successfully.\n",
      "# Target categories: 337\n",
      "First labels:\n",
      "  Accipiter gentilis\n",
      "  Accipiter nisus\n",
      "  Acrocephalus agricola\n",
      "  Acrocephalus arundinaceus\n",
      "  Acrocephalus dumetorum\n",
      "  Acrocephalus paludicola\n",
      "  Acrocephalus palustris\n",
      "  Acrocephalus schoenobaenus\n",
      "  Acrocephalus scirpaceus\n",
      "  Actitis hypoleucos\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"mobilenet-224-337wi-ft.tflite\" \n",
    "LABEL_FILE = \"species-list-337.txt\"\n",
    "TFLITE_THREADS = max(1, multiprocessing.cpu_count() // 2)\n",
    "\n",
    "interpreter = model_loading(MODEL_PATH, TFLITE_THREADS)\n",
    "LABELS = load_labels(LABEL_FILE)\n",
    "\n",
    "print(\"First labels:\")\n",
    "for k in LABELS[:10]:\n",
    "    print(f\"  {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e2b9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT FILE\n",
    "INPUT_PATH = \"./\"\n",
    "AUDIO_FILE = \"XC793531.MP3\"\n",
    "\n",
    "# PARAMETERS\n",
    "SAMPLE_RATE = 48000\n",
    "FILE_SPLITTING_DURATION = 600\n",
    "BANDPASS_FMIN = 0\n",
    "BANDPASS_FMAX = 15000\n",
    "SIG_LENGTH = 3.0\n",
    "SIG_OVERLAP = 0\n",
    "SIG_MINLEN = SIG_LENGTH\n",
    "MAX_LIMIT = 1000\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "rescaling = 1.0 / 255.0\n",
    "MAX_SEGMENTS = 1000\n",
    "MIN_CONF = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "958625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUXILIARY FUNCTIONS\n",
    "def apply_confidence_threshold(predictions, threshold=0.5):\n",
    "    filtered_predictions = {}\n",
    "    for k,pred in predictions.items():\n",
    "        max_prob = np.max(pred) \n",
    "        if max_prob >= threshold:\n",
    "            predicted_class = np.argmax(pred) \n",
    "            filtered_predictions[k]=(predicted_class, max_prob)\n",
    "    return filtered_predictions\n",
    "\n",
    "def analyze_file(f):\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "\n",
    "    print(f\"Analyzing {f}\", flush=True)\n",
    "    start_time = datetime.datetime.now()\n",
    "    full_path = os.path.join(INPUT_PATH, f)\n",
    "    chunk_preds = []\n",
    "    print_preds = {}\n",
    "\n",
    "    sig, rate = audio.openAudioFile(full_path, SAMPLE_RATE, offset=0, duration=FILE_SPLITTING_DURATION, fmin=BANDPASS_FMIN, fmax=BANDPASS_FMAX)\n",
    "    chunks = audio.splitSignal(sig, rate, SIG_LENGTH, SIG_OVERLAP, SIG_MINLEN)\n",
    "\n",
    "    for interval, y in enumerate(chunks[:MAX_SEGMENTS]):\n",
    "                    spec, _ = spectrogram(y, rate, shape=(128, 224))\n",
    "                    try:\n",
    "                        standardized_spec = (spec - np.min(spec)) / (np.max(spec) - np.min(spec)) \n",
    "                    except RuntimeWarning:\n",
    "                        continue\n",
    "\n",
    "                    spec_array = (np.asarray(standardized_spec.T) * 255)\n",
    "                    img = Image.fromarray(spec_array.T)\n",
    "\n",
    "                    # Preprocessing\n",
    "                    img = img.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "                    img = np.expand_dims(img, axis=-1)  # channel dimension (1)\n",
    "                    img = np.repeat(img, 3, axis=-1)  # to 3-channel\n",
    "                    img = np.expand_dims(img, axis=0)  # add batch dimension\n",
    "                    img = img.astype(np.float32) * rescaling\n",
    "\n",
    "                    # Model inference\n",
    "                    interpreter.set_tensor(input_details[0]['index'], img.astype(input_details[0]['dtype']))\n",
    "                    interpreter.invoke()\n",
    "\n",
    "                    # Results\n",
    "                    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                    predictions = np.squeeze(output_data)\n",
    "                    predicted_class = np.argmax(predictions)\n",
    "\n",
    "                    print_preds[f\"{interval*SIG_LENGTH}-{(interval+1)*SIG_LENGTH}\"] = predictions\n",
    "\n",
    "    # Show predictions\n",
    "    filtered_predictions = apply_confidence_threshold(print_preds, MIN_CONF)\n",
    "    for k, p_c in filtered_predictions.items():\n",
    "                        predicted_class, confidence = p_c\n",
    "                        label = LABELS[predicted_class] if predicted_class < len(LABELS) else f\"class_{pred_class}\"\n",
    "                        print(f\"{k.split('-')[0]}\\t{k.split('-')[1]}\\t{LABELS[predicted_class]}\\t{confidence:.2f}\\t{f}\")\n",
    "\n",
    "\n",
    "    delta_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    print(f\"Finished {f} in {delta_time:.2f} seconds\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f89ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing XC793531.MP3\n",
      "0.0\t3.0\tBubo bubo\t1.00\tXC793531.MP3\n",
      "3.0\t6.0\tBubo bubo\t1.00\tXC793531.MP3\n",
      "6.0\t9.0\tother\t0.92\tXC793531.MP3\n",
      "9.0\t12.0\tBubo bubo\t1.00\tXC793531.MP3\n",
      "12.0\t15.0\tBubo bubo\t1.00\tXC793531.MP3\n",
      "Finished XC793531.MP3 in 0.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- ANALYZE FILE ---------------------- #\n",
    "analyze_file(AUDIO_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
